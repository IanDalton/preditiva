{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/AnalisisPredictivo/blob/master/Kaggle/2023Q2/Un_primer_LR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uK-5cdUyNiQ3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "import cupy as cp\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP11tcaGOEfg"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train_OG=pd.read_csv('dataset/train.csv')\n",
        "\n",
        "X_train_OG.fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "y_train_OG = X_train_OG.averageRating\n",
        "\n",
        "\n",
        "X_train_OG.drop(['averageRating'], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "X_pred=pd.read_csv('dataset/val.csv')\n",
        "X_pred.fillna(0, inplace=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_OG, y_train_OG, test_size=0.1, random_state=881)\n",
        "\n",
        "#Show difference between train and test by columns\n",
        "#make a set of the columns that are in train but not in test\n",
        "\"\"\" \n",
        "print(set(X.columns))\n",
        "# Train the model on the GPU\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_score = 0\n",
        "best_random = 0\n",
        "best_regr = None\n",
        "randoms=set()\n",
        "scores = []\n",
        "for random in range(0, 20):\n",
        "    random = np.random.randint(0, 1000)\n",
        "    while random in randoms:\n",
        "        random = np.random.randint(0, 1000)\n",
        "    randoms.add(random)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_train_OG, y_train_OG, test_size=0.1, random_state=random)\n",
        "    regr = XGBRegressor(n_estimators=1100,eta=0.025,\n",
        "                        max_depth=19,gamma = 0.05,\n",
        "                        colsample_bytree = 0.7,colsample_bylevel=0.7,colsample_bynode=0.8,\n",
        "                        tree_method = 'hist',\n",
        "                        max_cached_hist_node=262144) # type: ignore\n",
        "    #regr = XGBRegressor(n_estimators=1200, eta=0.35, max_depth=7, multi_strategy=\"multi_output_tree\", min_child_weight=1, subsample=1, colsample_bytree=1, gamma=0, alpha=0)\n",
        "    print(\"Training a XGBRegressor\")\n",
        "    regr.fit(X_train, y_train)\n",
        "    print(\"Finished training the XGBRegressor\")\n",
        "\n",
        "    score = regr.score(X_test, y_test)\n",
        "\n",
        "    print(f\"R^2 score on testing {random} data: {score:.4f}\") \n",
        "    scores.append(score)\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_random = random\n",
        "        best_regr = regr\n",
        "    # 881 ~ 0.5570\n",
        "    # 791 ~ 0.5458\n",
        "    # 104 ~ 0.5386\n",
        "    # 0.942872\n",
        "\"\"\" \n",
        "      \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regr = XGBRegressor(n_estimators=1100,eta=0.025,\n",
        "                        max_depth=19,gamma = 0.20,\n",
        "                        colsample_bytree = 0.7,colsample_bylevel=0.7,colsample_bynode=0.8,\n",
        "                        tree_method = 'hist',\n",
        "                        max_cached_hist_node=262144) # type: ignore\n",
        "#regr = XGBRegressor(n_estimators=1200, eta=0.35, max_depth=7, multi_strategy=\"multi_output_tree\", min_child_weight=1, subsample=1, colsample_bytree=1, gamma=0, alpha=0)\n",
        "print(\"Training a XGBRegressor\")\n",
        "regr.fit(X_train, y_train)\n",
        "print(\"Finished training the XGBRegressor\")\n",
        "\n",
        "score = regr.score(X_test, y_test)\n",
        "\n",
        "print(f\"R^2 score on testing data: {score:.4f}\") \n",
        "# RMSE\n",
        "from sklearn.metrics import mean_squared_error\n",
        "y_pred = regr.predict(X_test)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"RMSE: %f\" % (rmse))\n",
        "#print(set(X_pred.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "\n",
        "# Create a linear support vector machine regressor object with parallel processing\n",
        "svr = LinearSVR(C=1.0, epsilon=0.1, max_iter=1000, dual=False, random_state=42, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Train the model on the training set\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "# Evaluate the model using root mean squared error\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error:\", rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regr = best_regr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "# Normalize the data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "X_train_norm = scaler.fit_transform(X_train)\n",
        "X_test_norm = scaler.transform(X_test)\n",
        "\n",
        "# Define the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_norm.shape[1], activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error', optimizer=Adam())\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_norm, y_train, epochs=100, batch_size=32, validation_data=(X_test_norm, y_test),workers=16)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Evaluate the model\n",
        "X_test = scaler.transform(X_test)\n",
        "score = model.evaluate(X_test, y_test)\n",
        "print(\"MSE on testing data: {:.4f}\".format(score))\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"RMSE: %f\" % (rmse))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    \"n_estimators\": [250, 500, 750,1100],\n",
        "    \"eta\": [0.025, 0.05, 0.1],\n",
        "    \"max_depth\": [15, 18, 20]\n",
        "}\n",
        "\n",
        "# Create a grid search object\n",
        "regr = XGBRegressor()\n",
        "grid_search = GridSearchCV(regr, param_grid, cv=5, n_jobs=6)\n",
        "\n",
        "# Train the grid search object on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and score\n",
        "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
        "print(\"Best score: {:.4f}\".format(grid_search.best_score_))\n",
        "\n",
        "# Evaluate the best model on the testing data\n",
        "best_regr = grid_search.best_estimator_\n",
        "score = best_regr.score(X_test, y_test)\n",
        "print(\"R^2 score on testing data: {:.4f}\".format(score)) \n",
        "\n",
        "# RMSE\n",
        "y_pred = best_regr.predict(X_test)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"RMSE: %f\" % (rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "param_grid = {\n",
        "    \"n_estimators\": [250, 500, 750,1100],\n",
        "    \"eta\": [0.025, 0.05, 0.1],\n",
        "    \"max_depth\": [15, 18, 20]\n",
        "}\n",
        "\n",
        "# Create a grid search object\n",
        "regr = XGBRegressor()\n",
        "grid_search = GridSearchCV(regr, param_grid, cv=5, n_jobs=6)\n",
        "\n",
        "# Train the grid search object on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and score\n",
        "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
        "print(\"Best score: {:.4f}\".format(grid_search.best_score_))\n",
        "\n",
        "# Evaluate the best model on the testing data\n",
        "best_regr = grid_search.best_estimator_\n",
        "score = best_regr.score(X_test, y_test)\n",
        "print(\"R^2 score on testing data: {:.4f}\".format(score)) \n",
        "\n",
        "# RMSE\n",
        "y_pred = best_regr.predict(X_test)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"RMSE: %f\" % (rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Create a LightGBM dataset\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "\n",
        "# Set the hyperparameters for the LightGBM model\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'num_leaves': 50,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 1,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 10,\n",
        "    'verbose': 1\n",
        "}\n",
        "\n",
        "# Train the LightGBM model\n",
        "num_rounds = 1000\n",
        "regr = lgb.train(params, train_data, num_rounds)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred = regr.predict(X_val)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Compute the mean squared error\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "\n",
        "# Compute the root mean squared error\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the root mean squared error\n",
        "print(\"RMSE: {:.2f}\".format(rmse))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define the number of threads to use\n",
        "#use max threads\n",
        "from random import randint\n",
        "num_threads = 16\n",
        "\n",
        "# Define the random states to use\n",
        "random_states = []\n",
        "for i in range(3):\n",
        "    random_states.append(randint(0, 1000))\n",
        "\n",
        "# Define the training and testing data\n",
        "\n",
        "\n",
        "# Define a function to train the model and return the R^2 score\n",
        "def train_model(random_state):\n",
        "    # Create a LinearForestRegressor with the specified random state\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state)\n",
        "    regr = XGBRegressor(n_estimators=1100,eta=0.025,max_depth=17,device=\"cuda\",tree_method=\"hist\",sampling_method=\"gradient_based\",subsample=0.6) # type: ignore\n",
        "    #regr = XGBRegressor(n_estimators=1200, eta=0.35, max_depth=7, multi_strategy=\"multi_output_tree\", min_child_weight=1, subsample=1, colsample_bytree=1, gamma=0, alpha=0)\n",
        "    print(\"Training a XGBRegressor\")\n",
        "    regr.fit(X_train, y_train)\n",
        "    print(\"Finished training the XGBRegressor\")\n",
        "    score = regr.score(X_test, y_test)\n",
        "\n",
        "    print(f\"R^2 score on testing data ({random_state}): {score:.4f}\") \n",
        "    return score\n",
        "\n",
        "# Use a ThreadPoolExecutor to run the function on multiple threads\n",
        "with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "    # Submit the function to the executor for each random state\n",
        "    futures = [executor.submit(train_model, random_state) for random_state in random_states]\n",
        "\n",
        "    # Wait for all the futures to complete and get the results\n",
        "    results = [future.result() for future in futures]\n",
        "\n",
        "# Print the average R^2 score across all the random states\n",
        "print(\"Average R^2 score across all random states: {:.4f}\".format(sum(results) / len(results)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lineartree import LinearForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "regr = LinearForestRegressor(base_estimator=LinearRegression(),max_depth=37,max_features=1.0,n_estimators=200) # type: ignore\n",
        "#regr = XGBRegressor(n_estimators=1200, eta=0.35, max_depth=7, multi_strategy=\"multi_output_tree\", min_child_weight=1, subsample=1, colsample_bytree=1, gamma=0, alpha=0)\n",
        "print(\"Training a XGBRegressor\")\n",
        "regr.fit(X_train, y_train)\n",
        "\n",
        "print(\"Finished training the XGBRegressor\")\n",
        "score = regr.score(X_test, y_test)\n",
        "\n",
        "print(\"R^2 score on testing data: {:.4f}\".format(score)) \n",
        "# 0.7 - 0.3819 | 0.6 - 0.3845 | 0.5 - 0.3867 | 0.4 - 0.3862\n",
        "# 50 - 0.3862 | 60 - 0.3875 | 70 - 0.3886 | 75 - 0.3888 | 80 - 0.3886\n",
        "# 5142\n",
        "# RMSE\n",
        "from sklearn.metrics import mean_squared_error\n",
        "y_pred = regr.predict(X_test)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"RMSE: %f\" % (rmse))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Assume regr is your trained LinearForestRegressor\n",
        "predictions = regr.predict(X_test).clip(1, 10)\n",
        "\n",
        "# Create a scatter plot of predicted vs actual values with transparency\n",
        "plt.scatter(y_test, predictions, alpha=0.05)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Actual vs Predicted Values')\n",
        "#Subtitle with score\n",
        "plt.text(0.05, 1.03, 'RMSE: {:.4f}'.format(rmse), ha='center', va='center', transform=plt.gca().transAxes)\n",
        "\n",
        "# Plot a line for perfect correlation. This serves as a reference line.\n",
        "plt.plot(y_test, y_test, 'r')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3USDelPp-Df"
      },
      "outputs": [],
      "source": [
        "Yest = regr.predict(X_pred).clip(1, 10)\n",
        "salida = pd.DataFrame(data={\"averageRating\": Yest})\n",
        "salida.index = X_pred.index\n",
        "salida.to_csv(\"predicciones/pred38.csv\", sep=',',index=True,  index_label='Id')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOWrEHZs9zDfmtIg+jWBoNC",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
