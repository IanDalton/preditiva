{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json,re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from preparando_datos import sum_into_column,split_and_sum,get_min_max,compute_average,sum_relevant_exp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "#TODO polars\n",
    "\n",
    "\n",
    "\n",
    "df_train=pd.read_csv('dataset/origen.csv')\n",
    "genre_types = df_train['genres_x'].unique().tolist()\n",
    "\n",
    "genre_types = [genre.split(\",\") if type(genre)==str else ['Missing'] for genre in genre_types]\n",
    "genre_types = [item for sublist in genre_types for item in sublist]\n",
    "genre_types = list(set(genre_types))\n",
    "\n",
    "\n",
    "\n",
    "status_types = df_train['status'].unique().tolist()\n",
    "status_types.remove(np.nan)\n",
    "status_types.append('Missing')\n",
    "status_types.append('Canceled')\n",
    "\"\"\" \n",
    "for i, status in enumerate(status_types):\n",
    "    status_types[i] = f\"status_{status}\" \"\"\"\n",
    "#take a 10% sample out of train\n",
    "df_train.rename(columns={'Unnamed: 0':'exp'}, inplace=True)\n",
    "\n",
    "df_test =  df_train.sample(frac=0.1, random_state=43)\n",
    "#drop the sample from train\n",
    "df_train.drop(df_test.index, inplace=True)\n",
    "\n",
    "X_test = df_test.drop([\"averageRating\"], axis=1)\n",
    "X = df_train.drop([\"averageRating\"], axis=1)\n",
    "#Rename 'Unnamed: 0' to 'exp'\n",
    "\n",
    "\n",
    "y = df_train[\"averageRating\"]\n",
    "y_test = df_test['averageRating']\n",
    "\n",
    "del df_test,df_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(eval(X['production_companies'].iloc[15])[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['runtimeMinutes'].replace(0, np.nan, inplace=True)\n",
    "X['budget'].replace(0, np.nan, inplace=True)\n",
    "X['revenue'].replace(0, np.nan, inplace=True)\n",
    "X['titleType'].replace(0, np.nan, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan \"[{'iso_3166_1': 'IT', 'name': 'Italy'}]\" '[]' ...\n",
      " \"[{'iso_3166_1': 'FI', 'name': 'Finland'}, {'iso_3166_1': 'EE', 'name': 'Estonia'}, {'iso_3166_1': 'KE', 'name': 'Kenya'}]\"\n",
      " \"[{'iso_3166_1': 'IN', 'name': 'India'}, {'iso_3166_1': 'SE', 'name': 'Sweden'}, {'iso_3166_1': 'RS', 'name': 'Serbia'}, {'iso_3166_1': 'GR', 'name': 'Greece'}]\"\n",
      " \"[{'iso_3166_1': 'BE', 'name': 'Belgium'}, {'iso_3166_1': 'FR', 'name': 'France'}, {'iso_3166_1': 'PT', 'name': 'Portugal'}]\"]\n"
     ]
    }
   ],
   "source": [
    "print(X.production_countries.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "categorical_columns_with_nan = [\n",
    "    c for c in X.columns if X[c].dtype == 'O' and X[c].isnull().any()]\n",
    "\n",
    "categorical_columns_without_nan = [\n",
    "    c for c in X.columns if X[c].dtype == 'O' and X[c].notnull().all()]\n",
    "\n",
    "\n",
    "\n",
    "numerical_columns_with_nan = [\n",
    "    c for c in X.columns if X[c].dtype != 'O' and X[c].isnull().any()]\n",
    "numerical_columns_without_nan = [\n",
    "    c for c in X.columns if X[c].dtype != 'O' and X[c].notnull().all()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformFromDict(BaseEstimator, TransformerMixin):\n",
    "    #Voy a intentar explicar esto lo mejor que puedas\n",
    "    def __init__(self, variables:list):\n",
    "        #\n",
    "        if type(variables) == str:\n",
    "            variables = [variables]     \n",
    "        self.variables = variables\n",
    "\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        def get_str(x):\n",
    "            #x is missing\n",
    "            if x is np.nan or x is None or x == 'Missing' or x == 'nan' or x == '[]' or x == '':\n",
    "                return ''\n",
    "            x = eval(x)\n",
    "            names = []\n",
    "            if type(x)== list:\n",
    "                for item in x:\n",
    "                    if type(item)==dict:\n",
    "                        name = item.get('name', '')\n",
    "                    else:\n",
    "                        name = ''\n",
    "                    names.append(name)\n",
    "            else:\n",
    "                names.append('')\n",
    "            return ','.join(names)\n",
    "        for group in self.variables:\n",
    "            X[group] = X[group].apply(get_str)\n",
    "        return X\n",
    "lista = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import AddMissingIndicator, CategoricalImputer, MeanMedianImputer\n",
    "from feature_engine.encoding import OneHotEncoder,RareLabelEncoder\n",
    "from feature_engine.selection import  DropFeatures\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "def interpolate_numericals(df:pd.DataFrame, group, category, method='linear'):\n",
    "    # Fill in missing values using interpolation \n",
    "    interpolated = df.groupby(group)[category].apply(lambda x: x.interpolate(method=method))\n",
    "    \n",
    "    # Reset the index of the interpolated DataFrame\n",
    "    interpolated = interpolated.reset_index(drop=True)\n",
    "    \n",
    "    # Replace the original column with the interpolated column\n",
    "    df[category] = interpolated\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "# Initialize the fitted values dictionary\n",
    "#interpolate_numericals.fitted_values = {}\n",
    "for to_remove in ['budget','revenue']:\n",
    "    try:\n",
    "        numerical_columns_with_nan.remove(to_remove)\n",
    "    except ValueError:\n",
    "        pass \"\"\"\n",
    "\n",
    "def check(x):\n",
    "    print(f\"Number of missing values in titleType before categorical imputer: {x['titleType'].isna().sum()}\")\n",
    "    return x\n",
    "class OverQuantileImputing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, variables:list, quantiles=4) -> None:\n",
    "        super().__init__()\n",
    "        self.quantiles = quantiles\n",
    "        self.variables = variables\n",
    "        self.thresholds = {}\n",
    "\n",
    "    def fit(self, X:pd.DataFrame, _y=None):\n",
    "        for var in self.variables:\n",
    "            self.thresholds[var] = []\n",
    "            for quantile in range(1, self.quantiles):\n",
    "                self.thresholds[var].append(X[var].quantile(quantile/self.quantiles))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        for var in self.variables:\n",
    "            X[f'{var}_quantile'] = 0\n",
    "            for i, threshold in enumerate(self.thresholds[var]):\n",
    "                X.loc[X[var] > threshold, f'{var}_quantile'] = i + 1\n",
    "        return X\n",
    "\n",
    "pipeline_preprocessing = Pipeline([\n",
    "    ('turnToNan', FunctionTransformer(lambda x: x.assign(runtimeMinutes=x['runtimeMinutes'].replace(0, np.nan),\n",
    "                                                          budget=x['budget'].replace(0, np.nan),\n",
    "                                                          revenue=x['revenue'].replace(0, np.nan),\n",
    "                                                          isAdult=x['isAdult'].apply(lambda x: 1 if x>1 else x),\n",
    "                                                          ))),\n",
    "    ('numericalMissingIndicator', AddMissingIndicator(variables=numerical_columns_with_nan)),\n",
    "   ('categoricalMissingIndicator', AddMissingIndicator(variables=categorical_columns_with_nan)),\n",
    "   ('quantileSignaler',OverQuantileImputing(variables=['runtimeMinutes','budget','revenue','numVotes'])),\n",
    "\n",
    "    ('runtimeTransform', FunctionTransformer(lambda x: interpolate_numericals(x, 'titleType', 'runtimeMinutes'))),\n",
    "    ('budgetTransform', FunctionTransformer(lambda x: interpolate_numericals(x, 'titleType', 'budget'))),\n",
    "    ('revenueTransform', FunctionTransformer(lambda x: interpolate_numericals(x, 'titleType', 'revenue'))),\n",
    "    ('endYearsTransform', FunctionTransformer(lambda X: X.assign(endYear=X.apply(lambda x: x['startYear'] if x['endYear'] == 0 else x['endYear'], axis=1)), validate=False)),\n",
    "    ('categoricalImputer', CategoricalImputer(variables=categorical_columns_with_nan,fill_value='Missing')),\n",
    "    ('fill_nans', FunctionTransformer(lambda x: x.assign(video = x['video'].replace('Missing',False),\n",
    "                                                          tagline = x['tagline'].replace('Missing',''),\n",
    "                                                          production_companies = x['production_companies'].replace('Missing',''),\n",
    "                                                            production_countries = x['production_countries'].replace('Missing',''),\n",
    "                                                          ))), \n",
    "    #('meanMedianImputer', MeanMedianImputer(imputation_method='median', variables=numerical_columns_with_nan)),\n",
    "    ('getSTRsFromDicts', TransformFromDict(variables=['production_companies','production_countries'])),\n",
    "    #('tvCategories',RareLabelEncoder(tol=0.01,n_categories=6, variables=['titleType'],replace_with='Other_TV')),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' pipeline_exp = Pipeline([\\n    \\n]) '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#la principal diferencia entre este y el anterior es que el anterior no divide por columnas, sino que muestran unicamente la experiencia en el tag relevante\n",
    "class GetExpDict(BaseEstimator, TransformerMixin):\n",
    "    #Voy a intentar explicar esto lo mejor que puedas\n",
    "    def __init__(self, group:list, categories:list,targets:list=[''],simple_only:list = [],simple_targets:list = []):\n",
    "        super().__init__()\n",
    "        if type(categories) == str:\n",
    "            categories = [categories]\n",
    "        if type(targets) == str:\n",
    "            targets = [targets]\n",
    "        group.extend(simple_only)\n",
    "        targets.extend(simple_targets)\n",
    "        self.group = group\n",
    "        self.categories = {category: [] for category in categories}\n",
    "        self.simple_only = simple_only\n",
    "        self.targets = targets\n",
    "        self.simple_targets = simple_targets\n",
    "\n",
    "    def process_group_category_target(self,group, category, target, X):\n",
    "        #print('Processing group:',group, category, target)\n",
    "        categorias = self.categories[category]\n",
    "        grupo = X.groupby(group).apply(lambda x: [(g, v) for g, v in zip(x[category], x[target] if target== 'exp' else x[category].value_counts())])\n",
    "        #grupo.to_json(f'grupo_{group}_{category}_{target}.json')\n",
    "        grupo = grupo.to_dict()\n",
    "        return (group, grupo, categorias, target)\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for category in self.categories:\n",
    "            categorias = X[category].unique().tolist()\n",
    "            categorias = [x.split(',') if type(x) == str else [] for x in categorias]\n",
    "            categorias = [item for sublist in categorias for item in sublist]\n",
    "            categorias = list(set(categorias))\n",
    "            self.categories[category] = categorias\n",
    "        \n",
    "        print(self.group, self.categories, self.targets)\n",
    "        print('creating parallel fitting job')\n",
    "        results = Parallel(n_jobs=-1)(delayed(self.process_group_category_target)(group, category, target, X) for group in self.group for category in self.categories for target in self.targets)\n",
    "        grupos = [result for result in results]\n",
    "        print('done')\n",
    "        self.diccionarios = Parallel(n_jobs=-1)(delayed(self._fit_group)(group, grupo, categorias,target) for group, grupo, categorias,target in grupos)\n",
    "        print('done')\n",
    "        return self\n",
    "    \n",
    "    def _fit_group(self, group, grupo, categorias, target):\n",
    "        # Create a dictionary with directors as keys and their shows as values\n",
    "        \n",
    "        dict_limpio = {}\n",
    "        for sub_grupo, shows in grupo.items():\n",
    "            for dir in sub_grupo.split(','):\n",
    "                dict_limpio[dir] = shows\n",
    "\n",
    "        # Initialize a dictionary to count categories\n",
    "        cat_counts = {}\n",
    "        for sub_grupo in dict_limpio.keys():\n",
    "            cat_counts[sub_grupo] = {}\n",
    "            for cat in categorias:\n",
    "                cat_counts[sub_grupo][cat] = [0, 0]\n",
    "\n",
    "        # Update the count for each category\n",
    "        for sub_grupo, values in dict_limpio.items():\n",
    "            for v in values:\n",
    "                cat = v[0]\n",
    "                count = v[1]\n",
    "                if cat in cat_counts[sub_grupo]:\n",
    "                    cat_counts[sub_grupo][cat][0] += count\n",
    "                    cat_counts[sub_grupo][cat][1] += 1\n",
    "\n",
    "        # Compute the average for each category and store it in authors_xp\n",
    "        authors_xp = {}\n",
    "        for sub_grupo, cat_dict in cat_counts.items():\n",
    "            authors_xp[sub_grupo] = {}\n",
    "            for cat, counts in cat_dict.items():\n",
    "                authors_xp[sub_grupo][cat] = compute_average(counts)\n",
    "        \n",
    "        if '0' in authors_xp:\n",
    "            del authors_xp['0']\n",
    "        if 'Missing' in authors_xp:\n",
    "            del authors_xp['Missing']\n",
    "        if '' in authors_xp:\n",
    "            del authors_xp['']\n",
    "        return group, authors_xp, categorias, target\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Define a helper function for parallel processing\n",
    "        def process_group(group, diccionario, categorias, target):\n",
    "            new_columns = {}\n",
    "            if group not in self.simple_only and target not in self.simple_targets:\n",
    "                new_columns.update({\n",
    "                    f'{group}_{target}_{category}': X[group].apply(sum_into_column, args=(diccionario, category,))\n",
    "                    for category in categorias\n",
    "                })\n",
    "            new_columns.update({\n",
    "                f'{group}_{target}_min': X[group].apply(get_min_max, args=(diccionario, 0, categorias,)),\n",
    "                f'{group}_{target}_max': X[group].apply(get_min_max, args=(diccionario, 1, categorias,)),\n",
    "                f'{group}_{target}_total': X[group].apply(sum_into_column, args=(diccionario, 'total',)),\n",
    "                f'{group}_{target}_relevant': X.apply(sum_relevant_exp, args=(diccionario,group,target), axis=1),\n",
    "            })\n",
    "            return new_columns\n",
    "\n",
    "        # Run the helper function in parallel\n",
    "        results = Parallel(n_jobs=1)(delayed(process_group)(group, diccionario, categorias, target) for group, diccionario, categorias, target in self.diccionarios)\n",
    "\n",
    "        # Combine the results\n",
    "        new_columns = {k: v for result in results for k, v in result.items()}\n",
    "\n",
    "        X = pd.concat([X, pd.DataFrame(new_columns)], axis=1)\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" pipeline_exp = Pipeline([\n",
    "    \n",
    "]) \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def okay(x):\n",
    "    print(\"Okay\")\n",
    "    return x\n",
    "def binarize_genres(X):\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    print('0')\n",
    "    binarized_genres = mlb.fit_transform(X['genres_x'].str.split(','))\n",
    "    df_binarized_genres = pd.DataFrame(binarized_genres, columns=mlb.classes_)\n",
    "    \n",
    "    print('1')\n",
    "    for column in genre_types:\n",
    "        if column not in df_binarized_genres.columns:\n",
    "            df_binarized_genres[column] = 0\n",
    "    print('2')\n",
    "    df_binarized_genres = pd.concat([X, df_binarized_genres], axis=1)\n",
    "    print('3')\n",
    "    return df_binarized_genres\n",
    "\n",
    "#TODO: Comparar a ver si es mejor ni usar order, season y episodeNumber\n",
    "class GetDetailedExp(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, group:dict, targets:dict):\n",
    "        super().__init__()\n",
    "        self.group = group\n",
    "        self.targets = targets\n",
    "\n",
    "    def process_group_target(self,group, target, X):\n",
    "        #print('Processing group:',group, category, target)\n",
    "        objetivos = {}\n",
    "        if self.targets[target]==0:\n",
    "            objetivos[f\"{group}_{target}\"] = X.groupby(group).count()[target].to_dict()\n",
    "\n",
    "        else:\n",
    "            for sub_target in self.targets[target]:\n",
    "                if sub_target:\n",
    "                    mask = X[target] == sub_target\n",
    "                    if mask.any():\n",
    "                        objetivos[f\"{group}_{target}_{sub_target}\"] = X[mask].groupby(group).count()[target].to_dict()\n",
    "                    else:\n",
    "                        print(f\"No rows where target == {sub_target}\")\n",
    "                else:\n",
    "                    objetivos[f\"{group}_{target}_other\"] = X[~X[target].isin(self.targets[target][:-1])].groupby(group).count()[target].to_dict()\n",
    "        \n",
    "        for grupo,diccionario in objetivos.items():\n",
    "            objetivos[grupo] = {\n",
    "                \"datos\" : split_and_sum(diccionario),\n",
    "                \"grupo\": group,\n",
    "            }\n",
    "            if '0' in objetivos[grupo][\"datos\"]:\n",
    "                del objetivos[grupo][\"datos\"]['0']\n",
    "            if 'Missing' in objetivos[grupo][\"datos\"]:\n",
    "                del objetivos[grupo][\"datos\"]['Missing']\n",
    "            if '' in objetivos[grupo][\"datos\"]:\n",
    "                del objetivos[grupo][\"datos\"]['']\n",
    "\n",
    "        \n",
    "        return objetivos\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print(self.group, self.targets)\n",
    "        print('creating parallel fitting job')\n",
    "        grupos = dict()\n",
    "        results = Parallel(n_jobs=-1)(delayed(self.process_group_target)(group, target, X) for group in self.group for target in self.targets)\n",
    "        grupos.update({group: result for dictionary in results for group, result in dictionary.items()})\n",
    "        print('done')\n",
    "        self.diccionarios = grupos\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "\n",
    "        def process_group(nombre,diccionario):\n",
    "            return {nombre:X[diccionario['grupo']].apply(sum_into_column, args=(diccionario['datos'],))}\n",
    "            \n",
    "        results = Parallel(n_jobs=-1)(delayed(process_group)(nombre, diccionario) for nombre,diccionario in self.diccionarios.items())\n",
    "\n",
    "        # Combine the results\n",
    "        new_columns = {k: v for result in results for k, v in result.items()}\n",
    "\n",
    "        X = pd.concat([X, pd.DataFrame(new_columns)], axis=1)\n",
    "        return X\n",
    "    \n",
    "\n",
    "\n",
    "pipeline = Pipeline([('preprocessing',pipeline_preprocessing),\n",
    "    ('set_exp_dict_genres', GetExpDict(group=['directors',\"writers\"],\n",
    "                                    categories=['genres_x','titleType'],\n",
    "                                    simple_only=['production_companies','production_countries'],\n",
    "                                    targets=['exp'],\n",
    "                                    simple_targets=['runtimeMinutes','numVotes'])),\n",
    "    ('set_detailed_exp',GetDetailedExp(group=['directors',\"writers\"],\n",
    "                                       targets={'seasonNumber':0,'episodeNumber':0,'titleType':['movie',None]})),\n",
    "\n",
    "    ('binarizeGenres', FunctionTransformer(binarize_genres)),\n",
    "\n",
    "    ('removeEmptyRows', FunctionTransformer(lambda x: x.dropna(subset=['directors']))),\n",
    "\n",
    "    \n",
    "    ('oneHotEncoder', OneHotEncoder(variables=['titleType',\"status\"])),\n",
    "    \n",
    "    ('get_all_companies',FunctionTransformer(lambda x: x.assign(production_companies=x['production_companies'].apply(lambda x: len(x.split(','))),\n",
    "                                                                production_countries=x['production_countries'].apply(lambda x: len(x.split(','))),\n",
    "                                                                directors=x['directors'].apply(lambda x: len(x.split(','))),\n",
    "                                                                writers=x['writers'].apply(lambda x: len(x.split(','))),\n",
    "                                                                tagline=x['tagline'].apply(len),\n",
    "                                                                video = x['video'].astype(int),\n",
    "                                                                budget = x['budget'].fillna(0),\n",
    "                                                                ))),\n",
    "\n",
    "    ('removeCategorical',FunctionTransformer(lambda x: x.drop(x.select_dtypes(include=['object']).columns, axis=1))), \n",
    "    ('removeExpColumn',FunctionTransformer(lambda x: x.drop(['exp'], axis=1))),\n",
    "\n",
    "\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['directors', 'writers', 'production_companies', 'production_countries'] {'genres_x': ['Fantasy', 'Missing', 'Talk-Show', 'Sport', 'Drama', 'History', 'Action', 'Romance', 'Western', 'Film-Noir', 'Animation', 'Comedy', 'Reality-TV', 'Mystery', 'Adult', '0', 'War', 'Musical', 'Adventure', 'Crime', 'Horror', 'Short', 'Music', 'News', 'Thriller', 'Game-Show', 'Family', 'Sci-Fi', 'Documentary', 'Biography'], 'titleType': ['tvSpecial', 'movie', 'tvSeries', 'tvMiniSeries', 'short', 'tvMovie', 'videoGame', 'tvEpisode', 'video', 'tvShort']} ['exp', 'runtimeMinutes', 'numVotes']\n",
      "creating parallel fitting job\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X,y) #4m 30.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_pred = pd.read_csv('dataset/testear.csv').drop(['Unnamed: 0'],axis=1)\n",
    "jjjj = pipeline.transform(X)\n",
    "\n",
    "#293 3m 42.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the target\n",
    "\"\"\" jjjj['averageRating'] = y\n",
    "jjjj.to_csv('dataset/transformedd.csv') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test2 = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "# import cupy as cp\n",
    "# import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = XGBRegressor(n_estimators=1100,eta=0.05,\n",
    "                        max_depth=18,gamma = 0.20,\n",
    "                        colsample_bytree = 0.7,colsample_bylevel=0.7,colsample_bynode=0.8,\n",
    "                        tree_method = 'hist',\n",
    "                        max_cached_hist_node=262144) # type: ignore\n",
    "#regr = XGBRegressor(n_estimators=1200, eta=0.35, max_depth=7, multi_strategy=\"multi_output_tree\", min_child_weight=1, subsample=1, colsample_bytree=1, gamma=0, alpha=0)\n",
    "print(\"Training a XGBRegressor\")\n",
    "regr.fit(jjjj, y)\n",
    "print(\"Finished training the XGBRegressor\")\n",
    "#Tarda 49 minutos con esos settings 4565\n",
    "regr.score(pipeline.transform(X_test),y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lrerg = LGBMRegressor(n_estimators=5000,learning_rate=0.2)\n",
    "lrerg.fit(pipeline.transform(X), y)\n",
    "lrerg.score(pipeline.transform(X_test),y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [1000, 1500, 2000],\n",
    "    'max_depth': [6, 8, 10,17,19,20],\n",
    "    'gamma':[0.1,0.2,0.3,0.5,0.7,1],\n",
    "    'min_child_weight': [1,2,3,4],\n",
    "}\n",
    "\n",
    "regr = XGBRegressor()\n",
    "grid_search = RandomizedSearchCV(regr, param_grid,n_iter=100, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(jjjj, y)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\"\"\" \n",
    "regr.fit(jjjj, y)\n",
    "regr.score(X_test,y_test) #8 ~ 0.4165 # 5000 y 4 = 0.3958 \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#X_test = pipeline.transform(X_test)\n",
    "print(\"Predicting\")\n",
    "score = regr.score(X_test, y_test) #4797\n",
    "\n",
    "print(f\"R^2 score on testing data: {score:.4f}\") \n",
    "# RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = regr.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE: %f\" % (rmse))\n",
    "#print(set(X_pred.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = pd.read_csv('dataset/testear.csv').rename(columns={'Unnamed: 0':'exp'})\n",
    "\n",
    "X_pred = pipeline.transform(X_pred)\n",
    "Yest = regr.predict(X_pred).clip(1, 10)\n",
    "salida = pd.DataFrame(data={\"averageRating\": Yest})\n",
    "salida.index = X_pred.index\n",
    "salida.to_csv(\"predicciones/pred40.csv\", sep=',',index=True,  index_label='Id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
